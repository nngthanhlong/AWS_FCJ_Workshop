[{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Announcing Capacity Blocks Support for AWS Parallel Computing Service By Kareem Abdol-Hamid and Kyle Bush on September 18, 2025 in AWS Parallel Computing Service, Compute, High Performance Computing\nThis article is a contribution by Kareem Abdol-Hamid and Kyle Bush\nToday, we are pleased to announce that Amazon EC2 Capacity Blocks for Machine Learning is now supported in AWS Parallel Computing Service (AWS PCS). This feature allows you to reserve and schedule Amazon EC2 accelerated GPU instances for future use, including NVIDIA Hopper GPU lines and AWS Trainium.\nAWS PCS is a managed service that makes it easy for you to run and scale HPC workloads and build scientific, engineering, or AI models on AWS using Slurm. With the addition of Capacity Blocks support, organizations can now ensure reserved access to accelerated computing resources when needed most, while still maintaining the simplicity of a fully managed service with operational responsibility for cluster management, updates, and monitoring.\nWhat are Capacity Blocks?\nCapacity Blocks is an EC2 feature that allows customers to reserve accelerated instances for future use by paying upfront. You can reserve blocks from 1 to 64 instances for a maximum duration of 6 months, with the ability to renew active reservations. This is particularly useful for organizations running large-scale training or inference workloads in AI or machine learning (ML), or GPU-accelerated code such as molecular dynamics or fluid dynamics, in HPC environments.\nKey Benefits\nReserved Access allows customers to reserve GPU instances up to 8 weeks in advance, ensuring resources are available for critical workloads while providing favorable pricing compared to on-demand rates. This feature fills the gap between flexible on-demand instances and long-term On-Demand Capacity Reservation (ODCR) commitments, making it ideal for customers running ML or HPC workloads that require consistent execution on powerful GPUs without long-term reservations. By allowing GPU resources to be reserved in advance, customers can maintain workflow continuity and optimize resource usage planning for scenarios such as periodic training cycles, scheduled model updates, and time-sensitive ML training jobs as well as research projects.\nFlexible Scheduling allows you to reserve GPU resources immediately for urgent tasks such as optimizing real-time inference, or schedule GPU jobs in advance on PCS for planned large-scale GPU workloads. With Slurm on PCS, you can queue jobs and they will start as soon as Capacity Block instances are ready. This flexibility ensures that you can access powerful instances when you need them, for example, to quickly fine-tune and validate models, or strategically schedule upcoming heavy workloads such as distributed training across multiple GPUs. Whether you are responding to immediate modeling needs or planning for large training pipelines, the ability to switch between immediate and future reservations ensures you always have access to the computing resources you need, while maintaining cost benefits and capacity reservations.\nCapacity Blocks also provides powerful resource sharing capabilities — allowing you to distribute reserved GPU instances across multiple PCS clusters. You can also combine multiple Capacity Blocks into a single large queue in PCS to maximize performance across different instance types. This helps maximize the value of reserved GPUs for diverse types of workloads or experiments. This capability also extends to sharing reserved capacity between multiple projects, ideal for organizations that want to efficiently allocate scheduled GPU resources between different initiatives or teams.\nSeamless Integration delivers a seamless experience by integrating Capacity Blocks directly into PCS Compute Node Groups (CNGs) through a simple purchase option. By simply updating the launch template, you can leverage all the familiar scheduling and queue management capabilities of PCS with reserved capacity. Groups can immediately use reserved GPU access while maintaining existing ML or HPC workflows and processes, making the switch to using Capacity Blocks as simple as selecting a new purchase option.\nGetting Started with Capacity Blocks in PCS\nBefore you get started, make sure you have created a PCS cluster (PCS cluster).\nCreate or Select a Capacity Block First, purchase your Capacity Block through the EC2 console, specifying:\nInstance type Number of instances (1-64) Duration (up to 6 months) Start date (up to 8 weeks in advance) Figure 1 – EC2 Capacity Blocks requirement dialog in AWS Management Console, showing instance type, capacity, duration, and start date.\nCreate Your Launch Template\nWe will create a launch template through the Amazon EC2 launch templates page.\nUpdate the instance type to match the instance type in your Capacity Block (in this case p5.4xlarge).\nFigure 2 – Create Launch Template dialog in AWS Management Console EC2, showing instance type matching the instance type specified when purchasing Capacity Block.\nIn Network settings, specify the Availability Zone corresponding to your Capacity Block (in this case us-west-2c) and select the security group you created during the PCS cluster setup process. You can leave other configurations as default or customize as preferred – they are not required to enable Capacity Blocks.\nFigure 3 – Create Launch Template dialog in AWS Management Console, showing Availability Zone matching Capacity Block\u0026rsquo;s AZ and security group matching PCS cluster\u0026rsquo;s security group. In Advanced Details, change the purchasing option to Capacity Blocks and under Capacity Reservation, select Specify Capacity Reservation. Choose your Reservation ID. Skip remaining details in Launch Template. This is the final step to enable Capacity Blocks.\nCreate Compute Node Group In the PCS Console, create a Compute Node Group by selecting your launch template and appropriate version. Update the instance profile if needed by creating a basic profile with permissions for EC2 instances to join AWS PCS cluster. Next, select the subnet corresponding to your Capacity Block and choose the Instance Type matching your Capacity Block, for example p5.4xlarge.\nFigure 5 – Create Compute Node Group dialog in AWS Management Console PCS, showing subnet matching Capacity Block\u0026rsquo;s subnet, instance type matching Capacity Block, and purchase option as \u0026ldquo;Capacity Block\u0026rdquo;. After a few minutes, you will see your instances running in the EC2 instances dashboard. When an instance passes the status check, it is ready to use.\nFigure 6 – EC2 Instances dashboard in AWS Management Console showing instance from Capacity Block launched by PCS and passed status check. Best Practices Tips\nWe recommend monitoring capacity utilization when sharing Capacity Blocks across multiple services using the console or CLI.\nTo change instance type or use a new Capacity Block, create a new compute node group instead of updating the existing one. This ensures smooth transition and avoids potential disruption to workloads.\nPlan job handling when Capacity Blocks expire: you can extend CB or alert groups through automatic EventBridge notifications. When extending CB, there is no disruption to jobs submitted to your PCS compute node group.\nTo identify expiring Capacity Blocks, note: EC2 will emit a Capacity Block Reservation Delivered event via EventBridge when a CB reservation starts, and a Capacity Block Reservation Expiration Warning 40 minutes before CB expires, with instances being reclaimed 30 minutes before CB expires. You can subscribe to these events and take appropriate action. More details are in the monitoring section of the official Capacity Blocks documentation.\nEnsure AZ (Availability Zone) matches between Capacity Block and compute node group (see Capacity Blocks launch guide).\nCapacity Blocks must be in scheduled or active status before connecting to PCS. If in scheduled status, PCS scheduler will keep jobs in queue until CB becomes active.\nWhen creating a compute node group, note that instances will not be launched until the start time of Capacity Reservation, even if the reservation was previously active.\nAvailability and Pricing\nPCS now supports Amazon EC2 Capacity Blocks in all AWS Regions where both services are available.\nStandard pricing for PCS and Capacity Block applies. You will be charged for reserved capacity according to the EC2 Capacity Blocks pricing model, regardless of usage level.\nHappy Building!\nTAGS: AI, Compute, GPU, HPC, Machine Learning, Research Computing, Scientific Computing\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Announcing Amazon EC2 M4 and M4 Pro Mac instances By Sébastien Stormacq on September 12, 2025 in Amazon EC2 Mac Instances, Launch, News Permalink Comments\nPermalink Comments Share Voiced by Polly\nHaving used macOS since 2001 and Amazon EC2 Mac instances since they launched 4 years ago, I have helped many customers scale their continuous integration \u0026amp; deployment (CI/CD) pipelines on AWS. Today, I am thrilled to share that M4 and M4 Pro Mac Amazon EC2 instances are now officially available.\nDevelopment teams building applications for Apple platforms need powerful computing resources to handle complex build processes and run multiple iOS simulators simultaneously. As development projects grow increasingly larger and sophisticated, teams need higher performance and memory capacity to maintain fast development cycles.\nApple M4 Mac mini at the Core\nThe EC2 M4 Mac instances (called mac-m4.metal in the API) are built on the Apple M4 Mac mini and use the AWS Nitro System. They feature Apple silicon M4 chip with 10 CPU cores (four performance and six efficiency cores), 10-core GPU, 16-core Neural Engine, and 24 GB unified memory, delivering improved performance for iOS and macOS application build workloads. When building and testing applications, M4 Mac instances deliver up to 20% better application build performance compared to EC2 M2 Mac instances.\nThe EC2 M4 Pro Mac instance (mac-m4pro.metal in the API) is equipped with Apple silicon M4 Pro chip with 14 CPU cores, 20 GPU cores, 16-core Neural Engine, and 48 GB unified memory. These instances provide up to 15% better application build performance compared to EC2 M2 Pro Mac instances. The increased memory capacity and compute power enable running multiple test iterations in parallel with multiple device simulators.\nEach M4 and M4 Pro Mac instance now comes with 2 TB of local storage, providing low-latency storage to improve caching and build \u0026amp; test performance.\nBoth instance types support macOS Sonoma version 15.6 and newer as Amazon Machine Images (AMIs). The AWS Nitro system provides Amazon Virtual Private Cloud (Amazon VPC) network bandwidth up to 10 Gbps and Amazon Elastic Block Store (Amazon EBS) storage bandwidth of 8 Gbps over high-speed Thunderbolt connections.\nEC2 Mac instances integrate seamlessly with AWS services, meaning you can:\nBuild automated CI/CD pipelines using AWS CodeBuild and AWS CodePipeline\nStore and manage multiple versions of your build secrets, such as Apple development certificates and keys, on AWS Secrets Manager\nManage your development infrastructure using AWS CloudFormation\nMonitor instance performance with Amazon CloudWatch\nHow to Get Started\nYou can launch an EC2 M4 or M4 Pro Mac instance through the AWS Management Console, AWS Command Line Interface (AWS CLI), or AWS SDKs.\nIn the demo example, I will start an M4 Pro instance from the console. I first allocate a dedicated host to run my instances. On the AWS Management Console I go to EC2, then Dedicated Hosts, and select Allocate Dedicated Host.\nThen, I enter a Name tag and select the instance Family (mac‑m4pro) and instance type (mac‑m4pro.metal). I select an Availability Zone and uncheck Host maintenance.\nEC2 Mac M4 – Dedicated hosts\nOr I can use CLI:\naws ec2 allocate-hosts \u0026ndash;availability-zone-id \u0026ldquo;usw2-az4\u0026rdquo; \u0026ndash;auto-placement \u0026ldquo;off\u0026rdquo; \u0026ndash;host-recovery \u0026ldquo;off\u0026rdquo; \u0026ndash;host-maintenance \u0026ldquo;off\u0026rdquo; \u0026ndash;quantity 1 \u0026ndash;instance-type \u0026ldquo;mac-m4pro.metal\u0026rdquo;\nAfter the dedicated host is allocated to my account, I select the newly allocated host, then select the Actions menu and choose Launch instance(s) onto host.\nNote that the console provides you, among other information, the latest supported macOS versions for this host type. In this case, macOS 15.6.\nOn the Launch an instance page, I enter the Name. I select a macOS Sequoia AMI. I ensure the Architecture is Arm 64-bit and the instance type is mac-m4pro.metal.\nThe remaining parameters are not specific to EC2 Mac: network configuration and storage. When launching an instance for development, be sure to select a volume of at least 200 GB or more. The default 100 GB volume is not enough to download and install Xcode.\nWhen ready, I click the orange Launch instance button at the bottom of the page. The instance will quickly appear in the Running state in the console. However, it may take up to 15 minutes before you can connect via SSH.\nOr I can use this command:\naws ec2 run-instances \u0026ndash;image-id \u0026ldquo;ami-000420887c24e4ac8\u0026rdquo; \\ # AMI ID varies by region ! \u0026ndash;instance-type \u0026ldquo;mac-m4pro.metal\u0026rdquo; \u0026ndash;key-name \u0026ldquo;my-ssh-key-name\u0026rdquo; \u0026ndash;network-interfaces \u0026lsquo;{\u0026ldquo;AssociatePublicIpAddress\u0026rdquo;:true,\u0026ldquo;DeviceIndex\u0026rdquo;:0,\u0026ldquo;Groups\u0026rdquo;:[\u0026ldquo;sg-0c2f1a3e01b84f3a3\u0026rdquo;]}\u0026rsquo; \\ # Security Group ID depends on your config \u0026ndash;tag-specifications \u0026lsquo;{\u0026ldquo;ResourceType\u0026rdquo;:\u0026ldquo;instance\u0026rdquo;,\u0026ldquo;Tags\u0026rdquo;:[{\u0026ldquo;Key\u0026rdquo;:\u0026ldquo;Name\u0026rdquo;,\u0026ldquo;Value\u0026rdquo;:\u0026ldquo;My Dev Server\u0026rdquo;}]}\u0026rsquo; \u0026ndash;placement \u0026lsquo;{\u0026ldquo;HostId\u0026rdquo;:\u0026ldquo;h-0e984064522b4b60b\u0026rdquo;,\u0026ldquo;Tenancy\u0026rdquo;:\u0026ldquo;host\u0026rdquo;}\u0026rsquo; \\ # Host ID depends on your config\n\u0026ndash;private-dns-name-options \u0026lsquo;{\u0026ldquo;HostnameType\u0026rdquo;:\u0026ldquo;ip-name\u0026rdquo;,\u0026ldquo;EnableResourceNameDnsARecord\u0026rdquo;:true,\u0026ldquo;EnableResourceNameDnsAAAARecord\u0026rdquo;:false}\u0026rsquo; \u0026ndash;count \u0026ldquo;1\u0026rdquo;\nInstalling Xcode from Terminal\nAfter the instance is accessible, I can connect via SSH and install development tools. I use xcodeinstall to download and install Xcode 16.4.\nFrom my laptop, I open a session with Apple developer credentials:\nI connect to the EC2 Mac instance I just launched. Then, I download and install Xcode:\nThings to Know\nSelect an EBS volume of at least 200 GB for development purposes. The default 100 GB volume is not enough to install Xcode. I typically choose 500 GB. When increasing EBS size after instance launch, remember to resize the APFS filesystem.\nAlternatively, you can choose to install your development tools and frameworks on the 2 TB low-latency internal SSD available in Mac mini. Note that the contents of this volume are attached to the instance lifecycle, not the dedicated host. This means everything will be deleted from the internal SSD when you stop and restart the instance.\nThe mac-m4.metal and mac-m4pro.metal instances support macOS Sequoia 15.6 and newer versions.\nYou can migrate existing EC2 Mac instances when the migration instance is running macOS 15 (Sequoia). Create a custom AMI from your existing instance and launch an M4 or M4 Pro instance from that AMI.\nFinally, I suggest you check the guides I wrote to help you get started with EC2 Mac:\nLaunch an EC2 Mac instance\nConnect to an EC2 Mac instance (I show you three different ways to connect)\nBuild your iOS applications faster with a CI/CD pipeline on EC2 Mac\nPricing and Availability\nEC2 M4 and M4 Pro Mac instances are now available in US East (N. Virginia) and US West (Oregon), with plans to expand to other regions in the future.\nEC2 Mac instances can be purchased as Dedicated Hosts under On-Demand pricing and Savings Plans. EC2 Mac billing is per second with a minimum of 24 hours allocation to comply with Apple\u0026rsquo;s macOS software license agreement. After the 24-hour minimum, the host can be released at any time without further commitment.\nAs someone working closely with Apple developers, I am curious to see how you will use these new instances to accelerate your development cycle. The combination of increased performance, improved memory capacity, and integration with AWS services opens up many possibilities for teams building applications for iOS, macOS, iPadOS, tvOS, watchOS, and visionOS. Beyond application development, the Neural Engine of Apple silicon makes these instances a cost-effective candidate for running machine learning (ML) inference workloads. I will discuss this topic in detail at AWS re:Invent 2025, where I will share benchmarks and best practices to optimize ML workloads on EC2 Mac.\nTo learn more about EC2 M4 and M4 Pro Mac instances, you can visit the Amazon EC2 Mac Instances page or consult the EC2 Mac documentation. You can start using these instances today to modernize your Apple development workflows on AWS.\nSébastien Stormacq\nSeb has been writing code since the Commodore 64 in the eighties. He inspires builders to unlock the value of AWS cloud, using a secret blend of passion, enthusiasm, customer advocacy, curiosity, and creativity. He is interested in software architecture, developer tools, and mobile computing. If you want to sell him something, make sure it has an API. Follow @sebsto on Bluesky, X, Mastodon, and other platforms.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Harnessing the Power of AWS IoT Rules with Substitution Templates By Andrea Sichel and Avinash Upadhyaya on September 17, 2025 in Advanced (300), Architecture, AWS IoT Core, AWS Lambda, Best Practices, Intermediate (200), Internet of Things, Technical How-to\nAWS IoT Core is a managed service that helps you securely connect billions of Internet of Things (IoT) devices to AWS cloud. The AWS IoT rules engine is a component of AWS IoT core that provides SQL-like capabilities to filter, transform, and decode your IoT device data. You can use AWS IoT rules to route over 20 AWS services and HTTP endpoints by using AWS IoT rule actions. Substitution templates are a capability in IoT rules that help enrich the JSON data returned when an IoT rule is triggered and AWS IoT performs an action. This blog post explores how AWS IoT rule actions with substitution templates unlock simpler, more powerful IoT architectures. You will learn proven ways to cut costs and increase scalability. Through real-world examples of message routing and load balancing, you will build smarter, more efficient IoT solutions.\nUnderstanding the Basic Components\nEach AWS IoT rule is built on three basic components: an SQL-like statement that handles message filtering and transformation, one or more AWS IoT rule actions that run and route data to other various AWS services and third-party services, and optional functions that can be used in both the SQL statement and rule actions.\nHere\u0026rsquo;s an example of an AWS IoT rule and its components.\nThe SQL statement acts as the rule processing gateway and determines which MQTT messages will be processed based on specific topic patterns and conditions. This rule uses a SQL-like statement and supports SELECT, FROM, and WHERE clauses (for more information, see AWS IoT SQL reference). In this structure, the FROM clause defines the MQTT topic filter, while the SELECT and WHERE clauses specify which data elements will be extracted or transformed from incoming messages.\nFunctions are essential to both SQL statements and rule actions in IoT rules. AWS IoT rules provide a rich collection of built-in functions designed to transform data types, manipulate strings, perform mathematical operations, process timestamps, and much more. Additionally, AWS IoT rules also provide external functions that help you retrieve data from AWS services (such as Amazon DynamoDB, AWS Lambda, Amazon Secrets Manager, and AWS IoT Device Shadow) and embed that data in your message payload. These functions support complex data transformations directly within the rule processing pipeline and eliminate the need for external processing.\nRule actions determine the destination and how processed data is handled. AWS IoT rules support a library of built-in rule actions that can transmit data to AWS services such as AWS Lambda, Amazon Simple Storage Service (Amazon S3), Amazon DynamoDB, and Amazon Simple Queue Service (Amazon SQS). These rule actions can also transmit data to third-party services like Apache Kafka. Each rule action can be configured with specific parameters that govern how data is delivered or processed by the destination service.\nSubstitution Templates: The Hidden Gem\nYou can implement functions in the SELECT and WHERE clauses of AWS IoT rules to transform and prepare message data. However, if you apply this approach too frequently, you might miss the powerful option of using substitution templates and performing transformations directly in the action of your IoT rule.\nSubstitution templates support dynamic insertion of rule values and functions into rule action JSON using the ${expression} syntax. These templates support many of the same functions as SQL statements, such as timestamp manipulation, encoding/decoding operations, string processing, and topic extraction. By using substitution templates in AWS IoT rule actions, you can implement sophisticated routing that significantly reduces complexity in other layers of your architecture, delivering simpler, more powerful AWS IoT solutions.\nReal-World Implementation Patterns\nLet\u0026rsquo;s explore some real-world examples that demonstrate the flexibility and power of using substitution templates in AWS IoT rule actions. These examples will illustrate how this feature can simplify IoT data processing workflows and unlock new possibilities for your IoT applications.\nExample 1: Conditional Message Distribution Using AWS IoT Registry Attributes\nConsider a common IoT scenario where a message distribution platform delivers device messages to different business partners, and each partner has their own SQS message processing queue. Different partners own each device in the fleet, and their relationship is maintained in the registry as an attribute called partnerId.\nThe traditional approach involves the following:\nOption 1 – Maintain partner routing logic on the device. Multiple AWS IoT rules based on WHERE conditions to input payload:\nRequires the device to know the partner\u0026rsquo;s ID. Increases device complexity and maintenance. Creates security concerns when exposing partner identifiers. Makes managing partner changes difficult. Option 2 – Use an intermediate Lambda function to retrieve the partner ID value associated with the device from the AWS IoT registry and then deliver the message to the partner\u0026rsquo;s specific SQS queue:\nAdds unnecessary compute and registry query costs. May increase message latency. Creates additional points of failure. Requires maintaining routing logic. May encounter Lambda concurrency limits. Here is a more sophisticated solution and process using substitution templates and the new AWS IoT attribute propagation feature:\nInsert Partner ID as an attribute in the AWS IoT registry.\nUse the attribute propagation feature to enrich your MQTTv5 user properties and dynamically build Amazon SQS queue URLs using the device\u0026rsquo;s partnerId. See the example below: Using this solution, a device with partnerId=\u0026ldquo;partner123″ will publish a message. This message will automatically be routed to the SQS queue \u0026ldquo;partner-queue-partner123\u0026rdquo;.\nBenefits of this solution:\nThe use of substitution templates significantly simplifies architecture and provides a scalable and maintainable solution for partner-specific message distribution. This solution,\nEliminates the need for additional compute resources. Provides instant routing without increasing latency. Simplifies managing partner relationships through updates in the AWS IoT registry. For example, introducing a new partner can be updated by modifying registry attributes. This update does not require any updates or changes to the device or routing logic. Maintains security by not exposing queue information to devices. Example 2: Intelligent Load Balancing with Amazon Kinesis Data Firehose\nConsider a scenario where millions of devices publish telemetry measurement data to the same topic. It is also necessary to distribute this large volume of data across multiple Amazon Data Firehose streams to avoid congestion issues when buffering data into Amazon S3.\nThe traditional approach includes:\nDevice-side load balancing:\nDeploy configuration management to provide different stream IDs across devices. Require devices to include the target stream in their messages. Create multiple AWS IoT rules to match specific stream IDs. AWS Lambda-based routing:\nDeploy a Lambda function to distribute messages across streams. Implement custom load balancing logic. Traditional approaches also create similar negative impacts as mentioned in the previous example (maintenance costs, security vulnerabilities, device complexity, additional costs, increased latency, and points of failure). Moreover, they pose specific challenges in large workload scenarios, such as the risk of high bandwidth throttling and complex stream management.\nBy leveraging AWS IoT rule substitution templates, you can implement a serverless, streamlined load balancing solution that dynamically assigns messages to different Firehose distribution streams by:\nCreating a random number from 0-100000 using rand()*100000. Converting (casting) this random number to an integer. Using the modulo (mod) operation to get the remainder when dividing by 8. Adding this remainder (0-7) to the \u0026ldquo;firehose_stream_\u0026rdquo; base name. The result is that messages are randomly distributed across eight different Amazon Data Firehose streams (firehose_stream_0 through firehose_stream_7). See the example below:\n{ \u0026#34;ruleArn\u0026#34;: \u0026#34;arn:aws:iot:us-east-1:123456789012:rule/testFirehoseBalancing\u0026#34;, \u0026#34;rule\u0026#34;: { \u0026#34;ruleName\u0026#34;: \u0026#34;testFirehoseBalancing\u0026#34;, \u0026#34;sql\u0026#34;: \u0026#34;SELECT * FROM \u0026#39;devices/+/telemetry\u0026#39;\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;createdAt\u0026#34;: \u0026#34;2025-04-11T11:09:02+00:00\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;firehose\u0026#34;: { \u0026#34;roleArn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:role/service-role/firebaseDistributionRoleDemo\u0026#34;, \u0026#34;deliveryStreamName\u0026#34;: \u0026#34;firehose_stream_${mod(cast((rand()*100000) as Int),8)}\u0026#34;, \u0026#34;separator\u0026#34;: \u0026#34;,\u0026#34;, \u0026#34;batchMode\u0026#34;: false } } ], \u0026#34;ruleDisabled\u0026#34;: false, \u0026#34;awsIotSqlVersion\u0026#34;: \u0026#34;2016-03-23\u0026#34; } } Benefits of this solution:\nThis flexible load balancing model helps process large volumes of messages by distributing the load across multiple streams. The key advantage of this approach lies in its scalability. By modifying the modulo function (which determines the remainder of division, for example: 5 mod 3 = 2), the divisor (currently set to 8) can be adjusted to match the desired number of streams. For example:\nChange to mod(…, 4) to distribute across 4 streams. Change to mod(…, 16) to distribute across 16 streams. Using this pattern makes it easy to scale your architecture up or down without changing the core logic of the rule.\nExample 3: Using CASE Statements in Substitution Templates to Build Conditional Routing Logic\nConsider a situation where you need to route your IoT device data, depending on the specific device, to either a production Lambda function or a development/testing (Dev/Test) Lambda function.\nThe traditional approach includes the following:\nDevice-side load balancing:\nDeploy configuration management to provide different environment IDs across devices. Require devices to include the environment ID in their messages. Create multiple AWS IoT rules to match specific environment IDs. AWS Lambda-based routing:\nDeploy a Lambda function to distribute messages to different AWS Lambda functions across environments after checking with the AWS IoT registry (or alternative database). Traditional approaches also have similar negative impacts as mentioned in previous examples.\nHere is a more sophisticated solution and process using substitution templates and the new AWS IoT attribute propagation feature:\nAssociate environment ID as an attribute to all devices in AWS IoT Registry Use the attribute propagation feature to enrich your MQTTv5 user properties Use propagated attributes to dynamically build the AWS Lambda function ARN in a CASE statement embedded in the AWS IoT Rule action definition. See the example below:\n{ \u0026#34;ruleArn\u0026#34;: \u0026#34;arn:aws:iot:us-east-1:123456789012:rule/ConditionalActions\u0026#34;, \u0026#34;rule\u0026#34;: { \u0026#34;ruleName\u0026#34;: \u0026#34;testLambdaConditions\u0026#34;, \u0026#34;sql\u0026#34;: \u0026#34;SELECT * FROM \u0026#39;devices/+/telemetry\u0026#39;\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;createdAt\u0026#34;: \u0026#34;2025-04-11T11:09:02+00:00\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;lambda\u0026#34;: { \u0026#34;functionArn\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:123456789012:function:${CASE get(get_user_properties(\u0026#39;environment\u0026#39;),0) WHEN \\\u0026#34;PROD\\\u0026#34; THEN \\\u0026#34;message_handler_PROD\\\u0026#34; WHEN \\\u0026#34;DEV\\\u0026#34; THEN \\\u0026#34;message_handler_DEV\\\u0026#34; WHEN NULL THEN \\\u0026#34;message_handler_PROD\\\u0026#34; ELSE \\\u0026#34;message_handler_PROD\\\u0026#34; END }\u0026#34;, } } ], \u0026#34;ruleDisabled\u0026#34;: false, \u0026#34;awsIotSqlVersion\u0026#34;: \u0026#34;2016-03-23\u0026#34; } } Benefits of this solution:\nThe use of substitution templates significantly simplifies architecture and provides a scalable and maintainable solution for partner-specific message distribution. This solution,\nEliminates the need to define separate IoT rules and rule actions for each condition. Helps you reduce the cost of using IoT rules and rule actions. Conclusion:\nThis blog post has explored how substitution templates for AWS IoT rules can transform complex IoT architectures into sophisticated and efficient solutions. The examples have demonstrated that substitution templates are not just a feature – they are a powerful architectural tool that leverages the capabilities of AWS IoT to effectively solve complex challenges without adding complexity or cost. Substitution templates provide a serverless, scalable approach that eliminates the need for additional computing resources or complex client-side logic. This approach not only reduces operational costs but also delivers immediate cost benefits by eliminating unnecessary computing resources and simplifying the overall architecture.\nThe next time you find yourself designing AWS IoT message routing patterns or facing scalability challenges, consider how substitution templates can provide a simpler and more efficient solution. By leveraging the powerful features of AWS IoT, you can create IoT solutions that are more maintainable, cost-effective, and scalable, truly serving your business needs.\nRemember: The simplest solution is often the most sophisticated one. With AWS IoT rule substitution templates, that simplicity is built in.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-lambda-basics/5.3.1-hello-node/","title":"Create a gateway endpoint","tags":[],"description":"","content":" Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://thienluhoan.github.io/workshop-template/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Thanh Long\nPhone Number: 0862330282\nEmail: Longntse181506@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-api-gateway/5.4.1-create-api/","title":"Prepare the environment","tags":[],"description":"","content":"To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Create an AWS account Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/09/2025 08/09/2025 https://policies.fcjuni.com/ 3 - Learn about AWS and its types of services + Create new AWS account + Enable MFA for AWS Account + Create Admin Group and Admin User + Support Account Authentication + Explore and configure AWS Management Console + Create and Manage Support Cases in AWS 09/09/2025 09/09/2025 https://000001.awsstudygroup.com/ 4 Hands-on Practice: + Create AWS account + Manage costs with AWS Budget + Request Support from AWS Support + Manage access permissions with AWS Identity and Access Management (IAM) 10/09/2025 10/09/2025 https://000001.awsstudygroup.com/ https://000007.awsstudygroup.com/ https://000009.awsstudygroup.com/ 5 - Learn how to draw AWS architecture on draw.io - Learn how to run AWS workshops 11/09/2025 11/09/2025 https://www.youtube.com/watch?v=l8isyDe-GwY\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=2 https://www.youtube.com/watch?v=mXRqgMr_97U\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=3 6 - Find and create group 12/09/2025 12/09/2025 Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions Create and manage key pairs Check information about running services Completed tasks to earn $200 credit\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: This week focuses on a deeper understanding of AWS networking services, starting from VPC and foundational components to advanced connectivity solutions and load balancing systems. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Understand VPC concepts and how AWS organizes virtual networks - Create a new VPC - Create public/private subnets - Set up Route Table and assign appropriate routes 15/09/2025 15/09/2025 https://000003.awsstudygroup.com/ 3 - Attach Internet Gateway to VPC - Configure routes for public subnet to Internet - Create NAT Gateway for private subnet - Check EC2 connectivity for public/private 16/09/2025 16/09/2025 https://000003.awsstudygroup.com/ 4 - Understand Security Group \u0026amp; Network ACL - Compare SG and NACL - Experiment: SG allow but NACL deny to observe results 17/09/2025 17/09/2025 https://000003.awsstudygroup.com/ 5 - Understand ALB, NLB, GWLB - Create ALB - Create 2 EC2 running simple web servers - Attach EC2 to Target Group - Check load balancing operations 18/09/2025 18/09/2025 https://000003.awsstudygroup.com/ 6 - Understand VPC Peering - Create 2 VPCs and establish Peering - Check communication between 2 EC2 in two VPCs - Learn more about Transit Gateway 19/09/2025 19/09/2025 https://000003.awsstudygroup.com/ Week 2 Achievements: Understood what VPC is and mastered the basic network components in AWS including:\nVPC Subnet (Public / Private) Route Table Internet Gateway (IGW) NAT Gateway Successfully created and configured a complete VPC:\nCreated a new VPC Created public and private subnets Assigned Route Tables to each subnet Set up routes to the Internet Connected VPC to the Internet through:\nInternet Gateway for public subnet NAT Gateway for private subnet Verified connectivity between EC2 instances in both subnets Understood AWS network security model:\nDifference between Security Group and Network ACL How to configure inbound/outbound rules Tested scenarios where SG allows but NACL blocks and vice versa Deployed basic load balancing system:\nCreated Application Load Balancer (ALB) Created 2 EC2 instances serving as web servers Created and configured Target Group Verified traffic distribution across EC2 instances Understood advanced VPC connectivity solutions:\nLearned about VPC Peering Created and configured Peering between 2 VPCs Verified communication between EC2 in different VPCs Grasped the role of Transit Gateway in large systems Able to build network architecture diagrams:\nDraw VPC diagrams ALB + EC2 diagrams VPC Peering diagrams Gained a comprehensive view of AWS networking and confident in operating important networking services.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: This week focuses on AWS Compute services, with emphasis on Amazon EC2 and related support services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Overview of AWS Compute services and EC2\u0026rsquo;s role - Prepare AWS account and IAM user for hands-on practice - Learn about instance types (General, Compute, Memory, GPU) - Understand AMI, snapshot, and EC2 backup methods 22/09/2025 22/09/2025 https://000004.awsstudygroup.com/ 3 - Launch Microsoft Windows Server 2022 EC2 instance - Launch Amazon Linux 2 EC2 instance - Practice RDP connection to Windows EC2 and SSH to Linux EC2 - Install basic packages on EC2 (e.g., Web Server, Node.js) 23/09/2025 23/09/2025 https://000004.awsstudygroup.com/ 4 - Practice basic EC2 management operations: + Start, Stop, Reboot, Terminate instance + Check instance status, public IP, private IP + Manage Security Groups, open necessary ports for applications 24/09/2025 24/09/2025 https://000004.awsstudygroup.com/ 5 - Deploy User Management Application on Amazon Linux 2: set up environment, upload code, run service - Deploy Node.js Application on Windows EC2: install Node.js, deploy, test - Check connectivity from local machine and web browser 25/09/2025 25/09/2025 https://000004.awsstudygroup.com/ 6 - Manage costs and access permissions with IAM: create policy, assign role, check user permissions - Monitor usage reports \u0026amp; budget to control costs - Clean up resources this week: terminate EC2, delete volumes, detach AMI, delete security groups 26/09/2025 26/09/2025 https://000004.awsstudygroup.com/ Week 3 Achievements: Understood the overall AWS Compute services and how to use related components:\nAmazon EC2 Auto Scaling Amazon Lightsail EFS, FSx AMI, Snapshot Mastered various EC2 instance types and specifications:\nGeneral Purpose, Compute Optimized, Memory Optimized, Storage Optimized, GPU vCPU, RAM, Burst, Network Performance parameters Proficient in EC2 operations:\nLaunch and manage Windows and Linux EC2 instances RDP/SSH connection and install basic packages Manage Security Groups, open necessary ports for applications Practice start, stop, reboot, terminate, and check instance status Understood and deployed AMI, snapshot, and backup:\nCreate custom AMI from EC2 Practice EBS snapshot Restore EC2 from AMI/snapshot Practiced basic Auto Scaling:\nCreate Launch Template Create Auto Scaling Group Verify automatic scale out/scale in capability Deployed applications on EC2:\nDeploy User Management Application on Amazon Linux 2 Deploy Node.js Application on Windows EC2 Verify connectivity from client and web browser, ensure stable operation Managed costs and access permissions:\nConfigure IAM, roles, policies Monitor usage reports \u0026amp; budget Identify potential costs and optimization methods Cleaned up resources after hands-on practice:\nTerminate EC2, delete volumes, detach AMI, delete Security Groups Keep environment clean for next week Comprehensive skills:\nUnderstand relationship between EC2, EBS, AMI, Auto Scaling Able to deploy, test, and manage Compute environment on AWS independently "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Grant application permissions to access AWS services with IAM Role Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Overview of how to grant access permissions to applications on AWS - Understand why you should not use access key/secret access key directly in applications 29/09/2025 29/09/2025 https://000048.awsstudygroup.com/ 3 - Hands-on practice using access key and secret access key in applications - Check ability to access AWS services (S3, EC2) from applications 30/09/2025 30/09/2025 https://000048.awsstudygroup.com/ 4 - Understand IAM Role on EC2 - Create IAM Role with appropriate policy - Attach IAM Role to EC2 instance - Experiment accessing AWS services without needing direct access key 01/10/2025 01/10/2025 https://000048.awsstudygroup.com/ 5 - Configure application to use IAM Role - Experiment accessing S3, DynamoDB or other services from application running on EC2 - Observe logs and confirm access permissions 02/10/2025 02/10/2025 https://000048.awsstudygroup.com/ 6 - Clean up resources created this week: delete IAM Role, policies, test EC2 - Document the entire permission granting process and lessons learned - Prepare knowledge for next week 03/10/2025 03/10/2025 https://000048.awsstudygroup.com/ Week 4 Achievements: Understood the mechanism of granting application access permissions on AWS:\nIAM Role concept and role in security Difference between using access key/secret access key and IAM Role Recognize risks when using access key directly in applications Practiced using access key/secret access key:\nConfigure in application Access AWS services like S3, EC2 Observe logs and debug when access permissions are insufficient Practiced IAM Role on EC2:\nCreate IAM Role with appropriate policy Attach IAM Role to EC2 instance Configure application to use IAM Role to access AWS services without access key Verify successful access and document results Managed permissions and policies:\nUnderstand how to create properly standards least-privilege policy Practice assigning policy to IAM Role Check access permissions for each service from the application Comprehensive skills:\nConfident in deploying applications on EC2 with secure access permissions Master best practices in managing AWS access permissions Document and clean up resources: delete IAM Role, policies, test EC2 "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Understand basic concepts of Amazon S3 and applications of object storage service. Create and manage buckets, configure access permissions. Deploy static website on S3 and optimize performance. Practice advanced features: Versioning, object movement, cross-region copy. Clean up resources after hands-on practice. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to Amazon S3 and use cases - Prepare bucket and AWS account - Learn about 11 9\u0026rsquo;s durability, availability, security and performance of S3 06/10/2025 06/10/2025 https://000057.awsstudygroup.com/ 3 - Enable Static Website feature on S3 - Configure Block Public Access and Public Object - Check website access from browser 07/10/2025 07/10/2025 https://000057.awsstudygroup.com/ 4 - Accelerate static website hosted on S3 (S3 Transfer Acceleration) - Practice configuring Bucket Versioning - Manage objects: upload, delete, move 08/10/2025 08/10/2025 https://000057.awsstudygroup.com/ 5 - Copy S3 objects to another region - Check accessibility and data synchronization - Practice advanced access permission management: IAM policy, Bucket policy 09/10/2025 09/10/2025 https://000057.awsstudygroup.com/ 6 - Clean up created resources: delete buckets, objects, reset policies - Document best practices when deploying S3 - Prepare knowledge for next week 10/10/2025 10/10/2025 https://000057.awsstudygroup.com/ Week 5 Achievements: Clearly understood Amazon S3 as an object storage service with scalability. Mastered creating, configuring and managing buckets: Block Public Access, Public Object, Bucket Policy, IAM Policy. Deployed static website on S3: Enable Static Website Hosting, verify access from browser, accelerate with Transfer Acceleration. Practiced advanced object management: Upload/download objects, Versioning, move objects, copy objects between regions. Managed access permissions and security: Configure IAM policy, Bucket policy, understand best practices when deploying S3. Cleaned up resources after hands-on practice and documented procedures. Able to apply S3 knowledge to subsequent labs and real projects. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Understand and proficiently use AWS CLI to manage AWS resources. Master the mechanism of profile, region configuration, output format. Practice interaction with: S3 SNS IAM VPC EC2 Understand basics of Amazon DynamoDB, NoSQL model, table, partition key, sort key. Practice complete CRUD and GSI on DynamoDB using Console \u0026amp; CloudShell. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Install AWS CLI + Download AWS CLI from AWS homepage + Install on Windows/macOS/Linux - Configure AWS CLI basic + Run aws configure command + Enter Access key ID, Secret access key, Region, Output format + Verify connection with aws s3 ls 13/11/2025 13/11/2025 https://000011.awsstudygroup.com/ 3 - Create and use profiles + Create additional profile besides default using --profile + Specify profile via AWS_PROFILE environment variable + Check profile with aws s3 ls --profile \u0026lt;profile_name\u0026gt; - Test AWS CLI with S3 + List buckets: aws s3 ls + Upload file: aws s3 cp file.txt s3://bucket-name/ + Download file: aws s3 cp s3://bucket-name/file.txt ./ 14/11/2025 14/11/2025 https://000011.awsstudygroup.com/ 4 - Practice AWS CLI with SNS + Create SNS topic: aws sns create-topic --name MyTopic + Subscribe subscriber: aws sns subscribe --topic-arn \u0026lt;ARN\u0026gt; --protocol email --notification-endpoint example@gmail.com + Send notification: aws sns publish --topic-arn \u0026lt;ARN\u0026gt; --message \u0026quot;Hello SNS\u0026quot; - AWS CLI with IAM + Create IAM user and group + Assign policy and access permissions + Check permissions with AWS CLI command 15/11/2025 15/11/2025 https://000011.awsstudygroup.com/ 5 - Amazon DynamoDB – Introduction + Create DynamoDB table + Add, edit, delete item + Enable automatic item deletion after expiration - DynamoDB – Backup \u0026amp; Restore + Create table backup + Restore table at specific point in time + Verify successful data restoration 16/11/2025 16/11/2025 https://000060.awsstudygroup.com/ 6 - Amazon DynamoDB – Advanced / Practice + Add secondary indexes (Global/Local Secondary Index) + Optimize query \u0026amp; scan + Check TTL (Time To Live) feature - Clean up DynamoDB resources + Delete unnecessary test tables + Ensure no cost incurrence 17/11/2025 17/11/2025 https://000060.awsstudygroup.com/ Week 6 Achievements: AWS CLI\nInstalled and configured basic AWS CLI. Entered Access Key ID, Secret Access Key, Region and Output format. Verified AWS connection with aws s3 ls. Created and used profiles, specified profile via environment variable. Practiced S3 management: list buckets, upload/download files. Practiced SNS: create topic, subscribe subscriber, send notification. Practiced IAM: create user and group, assign policy, check access permissions. Amazon DynamoDB\nFamiliarized with table, partition key, sort key. Practiced CRUD: add, edit, delete item. Enabled TTL (automatic item deletion after expiration). Created table backup and restored data. Added Global/Local Secondary Index. Optimized query \u0026amp; scan. Cleaned up test tables, ensured no cost incurrence. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Understand concepts and benefits of Amazon RDS compared to database on EC2. Deploy and manage DB instances with common engines (Aurora, MySQL, PostgreSQL, SQL Server, Oracle). Practice management features: Multi-AZ, Read Replicas, Snapshots, Backup \u0026amp; Restore. Manage security, data encryption, IAM Policy and VPC Subnet Group. Monitor, log and optimize DB performance. Clean up resources after hands-on practice. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to Amazon RDS and supported database management systems - Understand benefits of RDS compared to DB on EC2 - Initialize VPC and DB Subnet Group for DB instances 20/10/2025 20/10/2025 https://000005.awsstudygroup.com/ 3 - Create basic DB instance (MySQL or PostgreSQL) - Deploy Multi-AZ deployment - Test failover between AZs - Set up automatic backup configuration and maintenance window 21/10/2025 21/10/2025 https://000005.awsstudygroup.com/ 4 - Create and manage Read Replicas - Snapshot DB and restore from snapshot - Expand storage and change instance type - Check Multi-AZ and Read Replicas limitations 22/10/2025 22/10/2025 https://000005.awsstudygroup.com/ 5 - Set up DB security: IAM Role, KMS encryption, Security Group, Public/Private endpoints - Practice data migration with AWS DMS \u0026amp; SCT 23/10/2025 23/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Monitor and optimize RDS: + CloudWatch Metrics \u0026amp; Alarms: monitor CPU, IOPS, Storage, DatabaseConnections + Enhanced Monitoring: detailed operating system monitoring + Performance Insights: analyze queries and optimize + Check logs, RDS Events \u0026amp; CloudTrail 24/10/2025 24/10/2025 https://000005.awsstudygroup.com/ 6 - Clean up DB instances and resources: + Delete DB instances, Multi-AZ, Read Replicas + Delete DB Snapshots (manual and automatic) + Delete DB Subnet Groups + Delete related Security Groups + Check on Console \u0026amp; CLI, document cleanup process 24/10/2025 24/10/2025 https://000005.awsstudygroup.com/ Week 7 Achievements: Clearly understood Amazon RDS as a managed relational database service, different from DB on EC2. Proficiently deployed DB instances with common engines: MySQL, PostgreSQL, Aurora, SQL Server, Oracle. Used Multi-AZ to increase availability and successfully tested failover. Managed Read Replicas and Snapshots to expand read capability and data recovery. Practiced expanding storage and changing instance type without service interruption. Deployed DB security: IAM Role, KMS encryption, Security Group and managed Public/Private endpoints. Practiced data migration with AWS DMS \u0026amp; SCT. Monitored RDS performance and logs via CloudWatch, Performance Insights, Enhanced Monitoring and CloudTrail. Cleaned up resources and documented complete hands-on procedures. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Clearly understand the architecture and operating mechanism of Amazon S3 (Bucket – Object – Storage Class). Master important S3 features: Versioning, Encryption, Lifecycle, Replication. Practice data management on S3: upload, permission assignment, public/private, static website hosting. Apply best practices for security and cost optimization when working with S3. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn overview of Amazon S3: + What is S3 + Bucket – Object architecture + Availability and storage models 27/10/2025 27/10/2025 https://000057.awsstudygroup.com/1-introduce/ 3 - Analyze S3 Bucket vs Object + URL structure + Prefix \u0026amp; virtual folder + Metadata, Key, ACL - Classify Storage Classes 28/10/2025 28/10/2025 https://000057.awsstudygroup.com/1-introduce/ 4 - Research S3 security features: + Encryption (SSE-S3, SSE-KMS) + IAM Policy, Bucket Policy + Block Public Access - Understand Versioning 29/10/2025 29/10/2025 https://000057.awsstudygroup.com/1-introduce/ 5 - Learn about Lifecycle Rules \u0026amp; Replication: + Change storage class + CRR \u0026amp; SRR + Delete Marker - S3 Storage Lens \u0026amp; cost analysis 30/10/2025 31/10/2025 https://000057.awsstudygroup.com/1-introduce/ 6 Hands-on Practice: + Create bucket + Upload \u0026amp; manage objects + Enable/disable public access + Static website hosting + Configure versioning \u0026amp; lifecycle + Test object access 31/10/2025 31/10/2025 https://000057.awsstudygroup.com/1-introduce/ Week 8 Achievements: Clearly understood Amazon S3 foundation, including:\nBucket–object architecture Object metadata, prefix, virtual folder URL structure to access objects Mastered S3 storage classes:\nStandard Standard-IA Intelligent-Tiering Glacier / Deep Archive Applied important features proficiently:\nVersioning Lifecycle Management Replication (CRR, SRR) Block Public Access Encryption (SSE-S3, SSE-KMS) Complete hands-on practice on S3:\nCreate bucket Upload \u0026amp; assign permissions to objects Static website hosting Configure versioning, lifecycle Verify public/private object access via URL Mastered best practices:\nDon\u0026rsquo;t make buckets public Use default encryption Optimize costs with lifecycle \u0026amp; storage class Organize data by prefix to increase performance "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Understand architecture and operating mechanism of Amazon Route 53, especially Hybrid DNS integrating with on-premise DNS systems. Proficiently use CloudWatch to: Collect logs \u0026amp; metrics Create alarms Build monitoring dashboard Monitor containers with Container Insights Master centralized monitoring model and automated reaction automation based on events. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Route 53 – DNS Architecture \u0026amp; Hybrid DNS - Review traditional DNS model (Authoritative, Recursive). - Learn overview of Amazon Route 53: • Public Hosted Zone • Private Hosted Zone • DNS resolution process on AWS. - Analyze on-premise DNS and Hybrid DNS needs. - Learn Route 53 Resolver and its role. - Review common records: A, AAAA, CNAME, Alias… - Draw DNS operation diagram before and after Route 53 integration. 03/11/2025 03/11/2025 https://000010.awsstudygroup.com/ 3 Route 53 – Hybrid DNS - Learn Outbound Endpoints: how AWS sends queries to on-premise. - Learn Inbound Endpoints: point to receive queries from on-premise. - Configure Route 53 Resolver Rules: • Forwarding rule • Conditional rule - Demo: create trial Outbound + Inbound endpoint. - Test name resolution between AWS ↔ on-premise. - Summarize common errors: timeout, loop, rule conflict, network unreachable. 04/11/2025 04/11/2025 https://000010.awsstudygroup.com/ 4 AWS CloudWatch Workshop – Metrics \u0026amp; Logs - Understand CloudWatch monitoring architecture. - Analyze Metrics structure: Namespace, Dimension, Retention. - Distinguish Logs: Log group, Log stream, Log event. - Demo: • Install CloudWatch Agent on EC2. • Send system logs to CloudWatch. • View metrics CPU, RAM, Network. - Learn how to optimize CloudWatch costs. 05/11/2025 05/11/2025 https://000008.awsstudygroup.com/ 5 AWS CloudWatch Workshop – Alarms \u0026amp; Automation - Create Metric Alarm with custom threshold. - Create Log Filter → Log Alarm. - Integrate SNS to send notifications. - Create Composite Alarm with multiple conditions. - Real-world simulation exercises: • CPU \u0026gt; 70% → send alert → Auto Scaling scale-out. • EC2 log ERROR → send notification via email. 06/11/2025 06/11/2025 https://000008.awsstudygroup.com/ 6 AWS CloudWatch Workshop – Dashboard \u0026amp; Container Insights - Create CloudWatch Dashboard displaying: • CPU, RAM, Network • ALB request count • RDS metrics - Design dashboard following best practice (group panels, annotate, colors). - Learn EventBridge \u0026amp; its role in automation. - Container Insights: • Collect metrics ECS/EKS. • Investigate container restart, OOMKilled errors. - Clean up all resources. 07/11/2025 07/11/2025 https://000008.awsstudygroup.com/ Week 9 Achievements: 1. Route 53 \u0026amp; Hybrid DNS\nClearly understood traditional DNS model and DNS in AWS environment. Mastered components in Route 53: Public Hosted Zone Private Hosted Zone Record types (A, AAAA, CNAME, Alias) Route 53 Resolver Understood Hybrid DNS architecture between on-premise and AWS. Configured and tested: Outbound Endpoint for AWS to send queries to on-premise DNS. Inbound Endpoint for on-premise to send queries to AWS. Resolver Rules (Forwarding rule / Conditional rule). Practiced bi-directional DNS resolution: EC2 in VPC → DNS on-premise. DNS on-premise → Private Hosted Zone of AWS. Know how to handle common errors: DNS timeout Forwarding loop Wrong rule priority Endpoint unable to attach to VPC Self-draw and analyze complete Hybrid DNS architecture diagram. 2. CloudWatch Metrics \u0026amp; Logs\nUnderstood CloudWatch monitoring architecture and its role in AWS system. Mastered how Metrics work: Namespace Dimension Statistic (Average, Sum, Maximum) Metric retention 15 months Collected Logs through: CloudWatch Log Groups Log Streams Log Events Installed and configured CloudWatch Agent to send: System logs Application logs Custom metrics (CPU, RAM, Disk, Network) 3. CloudWatch Alarm \u0026amp; Automation\nCreated and configured: Metric Alarm with CPU, Network thresholds… Log Metric Filter to detect Error/Warning in logs. Log Alarm based on filter pattern. Integrated Alarm with SNS: Send email alerts. Send notifications when resources exceed thresholds. Implemented Automation model: CPU \u0026gt; 70% → send alert → trigger Auto Scaling Scale Out. ERROR in application → send notification immediately. Able to analyze root cause via metrics/logs to reduce MTTR. 4. CloudWatch Dashboard \u0026amp; Container Insights\nCreated and customized Dashboard to monitor: EC2 (CPU, RAM, Network) ALB (Request count, Target health) RDS (Latency, Connections) Custom metrics Knew how to organize widgets scientifically by module. Used Container Insights to monitor containers: CPU/memory usage by pod/task Container restart, OOMKilled errors Deployment, node health statistics Understood standard DevOps/SRE monitoring operation: Observability Alerting Distributed logs Telemetry data pipeline 5. Resource management skills\nPerformed resource cleanup correctly to avoid cost incurrence. Able to recognize resources causing CloudWatch cost: Metric storage Log ingestion High-resolution metrics Dashboard widgets "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/","title":"Worklog","tags":[],"description":"","content":"This is a learning journal tracking my AWS learning journey, starting from September 8, 2025. The documentation records learning progress at each stage and important topics in the learning roadmap.\nThe worklog spans 3 months — corresponding to the internship period — and is divided into weekly sections with the following content:\nPhase 1: Foundations (Weeks 1-3) Week 1: Getting familiar with AWS and basic services Week 2: Networking Fundamentals (VPC, Subnet, Security) Week 3: Amazon EC2 and related support services Phase 2: Management \u0026amp; Storage (Weeks 4-6) Week 4: Granting application permissions to access AWS services with IAM Role Week 5: Understanding and practicing S3 Week 6: Databases: RDS, Aurora, DynamoDB Phase 3: Security \u0026amp; Serverless (Weeks 7-8) Week 7: Security \u0026amp; Identity Management: IAM, KMS, Organizations Week 8: Serverless: Lambda, API Gateway, SQS, SNS Phase 4: Advanced \u0026amp; Deployment (Weeks 9-11) Week 9: Containers: ECS, ECR and application deployment Week 10: Monitoring \u0026amp; Observability: CloudWatch, X-Ray Week 11: Advanced architecture design \u0026amp; cost optimization Phase 5: Completion (Week 12) Week 12: Final project \u0026amp; internship summary "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary: \u0026ldquo;AWS CloudDay 2025\u0026rdquo; Time: 09:00 – 18/09/2025\n1. Purpose of the Event Update on the latest trends in Cloud, AI/GenAI, and digital transformation. Share real-world experience from major enterprises in Vietnam. Introduce new AWS solutions and their practical applications. Connect the technical community, AWS experts, and businesses. 2. Notable Speakers List Pham Duc Long – Vice Minister of Science \u0026amp; Technology Marc E. Knapper – US Ambassador to Vietnam Eric Yeo – General Director AWS VN, Cambodia, Laos, Myanmar Dr. Jens Lottner – CEO Techcombank Trang Phung – CEO \u0026amp; Co-founder U2U Network Jaime Valles – VP, Managing Director APJ, AWS Many experts from AWS \u0026amp; major enterprises (TechX, TymeX, Bảo Việt, Masterise, Honda, Mobifone,\u0026hellip;) 3. Event Highlights Morning Full Session Registration \u0026amp; Opening ceremony Remarks from the Ministry of Science \u0026amp; Technology and US Embassy Keynote on AWS Cloud \u0026amp; GenAI Strategy Case studies from Techcombank \u0026amp; U2U Network Panel: Business Restructuring with Cloud \u0026amp; GenAI Afternoon – 4 Specialized Tracks Track 1A – FSI Banking / Finance Track 1B – FSI Insurance Track 2 – Cross-Industry Track 3 – AI \u0026amp; Next-Gen Data Track 4 – Migration \u0026amp; Modernization 4. Learning Outcomes Overview of AWS solution ecosystem across various sectors. Modern Cloud architecture according to Well-Architected Framework. How to build and deploy GenAI/AI Agents at enterprise scale. Understanding of Data Platform, EKS, GPU, Bedrock, XGenAI. Grasping modernization models and system optimization. 5. Application to Work Deploy GenAI to optimize internal operations. Improve architecture design according to AWS standards. Increase DevOps productivity with AI support. Apply Bedrock for chatbots, automation, and data analysis. Adopt \u0026ldquo;Cloud-first, Modernization-first\u0026rdquo; mindset. 6. Event Experience Attend a grand keynote with thousands of guests. Direct engagement with international AWS experts. Experience AI technology demos, Data Lakes, GPU, EKS. Many experience booths, networking activities, Q\u0026amp;A sessions. 7. Learning from Highly Specialized Speakers How to operate Cloud \u0026amp; AI at major companies. Leadership mindset in digital transformation: People – Culture – Innovation. Practical and feasible AI/GenAI strategies. How businesses solve cost – performance – security challenges. 8. Hands-On Technical Experience Demo AI Agents \u0026amp; AI Factory (Bedrock). EKS + GPU architecture for large-scale applications. Digital banking, digital insurance, and lending solutions. QA automation with GenAI. 9. Application of Modern Tools Amazon Bedrock – Enterprise GenAI Platform EKS / ECS – Container Orchestration XGenAI – AI Solution from TechX SageMaker – AI/ML Pipeline CloudWatch / X-Ray – Monitoring \u0026amp; Observability Data Lakehouse / Redshift / Glue – Unified Data Systems 10. Networking and Exchange Meet AWS experts, CTOs, CEOs, and architects. Learn from real-world cloud project deployment experience. Expand networking within the Cloud/AI community in Vietnam. 11. Key Takeaways GenAI is an inevitable trend, requiring clear strategy + governance. Digital transformation must go hand-in-hand with modernization, not just migration. Every industry can apply AI — banking, insurance, manufacturing, etc. Data is the most important foundation for AI to deliver effectiveness. Community \u0026amp; continuous practice help improve Cloud/AI skills. "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary: \u0026ldquo;Generative AI with Amazon Bedrock\u0026rdquo; Time: 08:30 – 15/11/2025\nPurpose of the Event Provide foundational knowledge about Generative AI and highlight differences compared to traditional Machine Learning approaches. Introduce Amazon Bedrock service in detail along with supported Foundation Models. Present the RAG (Retrieval Augmented Generation) technical process to build smarter, more accurate AI applications and reduce hallucination phenomena. Overview the ecosystem of specialized AI services on AWS and how they integrate with Bedrock. Speakers List Lam Tuan Kiet - Sr DevOps Engineer, FPT Software. Danh Hoang Hieu Nghi - AI Engineer, Renova Cloud. Dinh Le Hoang Anh - Cloud Engineer Trainee, First Cloud AI Journey. Event Highlights Transition: From Traditional ML to Foundation Models Traditional Machine Learning models operate in a \u0026ldquo;one model – one task\u0026rdquo; fashion, requiring large amounts of labeled data and complex training and deployment processes.\nFoundation Models (FM) are trained on massive amounts of unlabeled data, providing better generalization and applicability to various tasks such as text generation, summarization, question-answering, and chatbots.\nAWS AI Ecosystem Amazon Bedrock is a hub for advanced models from AI21, Anthropic, Cohere, Meta, Stability AI, and Amazon\u0026rsquo;s own models.\nBeyond FMs, AWS provides a series of Specialized AI Services – ready-to-use services without needing self-training:\nAmazon Rekognition: Image and video recognition and analysis. Amazon Translate: Real-time language translation. Amazon Textract: Extract structured data from scanned documents. Amazon Transcribe: Convert audio to text. Amazon Polly: Convert text to speech. Amazon Comprehend: Sentiment analysis, topic detection, keyword extraction. Amazon Kendra: Natural language search within enterprise documents. Amazon Lookout: Detect anomalies in industrial systems. Amazon Personalize: Build real-time recommendation systems. Prompting Technique: Chain of Thought (CoT) Comparison between Standard Prompting (asking and getting a direct answer) and Chain-of-Thought Prompting. CoT encourages the model to think out loud, breaking down problems into multiple reasoning steps. This approach is particularly effective for complex logical problems and often yields more accurate and consistent results compared to direct question answering. RAG (Retrieval Augmented Generation) The Problem: LLMs are often limited by their training data (not continuously updated) and sometimes produce hallucinations. The Solution: Combine LLMs with an external Knowledge Base retrieval layer. The model retrieves relevant information and uses it to construct answers – rather than relying solely on memorized knowledge. Data Ingestion Process: Collect new data and divide it into chunks. Pass each chunk through an Embeddings model (e.g., Amazon Titan Text Embeddings V2.0) to convert to vectors. Store these vectors in a Vector Store (OpenSearch Serverless, Pinecone, Redis, etc.). RetrieveAndGenerate API: Handles the entire pipeline: receives user input → creates embedding for the query → queries Vector Store → injects context into prompt → calls LLM to generate final answer. Learning Outcomes About AI and Cloud Better understand when to use specialized AI services – suitable for quick and specific tasks – versus when to use Generative AI or Bedrock for creative problems or high customization needs. Understand RAG thinking, which focuses more on data organization and providing proper context rather than just calling an LLM API. Technical Architecture Better understand how an AI system works end-to-end. Not just \u0026ldquo;call the model to generate an answer,\u0026rdquo; but the system requires many data preparation and information organization steps before AI can answer accurately.\nRAG architecture is broken down into clear steps: data collection → chunking → storage in search repository → retrieval of relevant information → feeding into the model for answering. This step-by-step approach makes management and scaling easier as data grows.\nApplication to Work Standardize unstructured data: Use Textract and Bedrock Agents to automate extraction of information from forms, invoices, and scanned documents, minimizing manual work.\nSupport decision-making: Apply generative models to automatically summarize reports, highlight key metrics, and suggest actions based on current data.\nEvent Experience Attending the workshop was very valuable, providing me a comprehensive view of AI and how to use it optimally and reasonably. The game section was also very helpful in reinforcing the knowledge learned during the workshop.\nNetworking and Exchange The workshop created opportunities for direct discussions with speakers, helping me understand more about AI usage and some new AI tools. After each presentation, I had access to the speakers\u0026rsquo; LinkedIn profiles to continue learning afterward. Key Takeaways Attending the workshop gave me a clearer perspective on how to apply AI effectively. I realized that RAG is an important component for AI to provide accurate answers based on enterprise-internal data.\nAdditionally, I was impressed by AWS\u0026rsquo;s comprehensive ecosystem — from storage, models, to orchestration tools — making building a real AI application much easier and more feasible.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.1-workshop-overview/","title":"Overview","tags":[],"description":"","content":"Objectives Introduce the serverless workshop: creating Lambda functions (Node.js/Python), receiving input parameters, integrating API Gateway, and deploying an in-app purchase recommendation API. Emphasize how to reduce deployment time using Lambda without requiring server management.\nGeneral Architecture Lambda receives events, processes logic, returns JSON. API Gateway acts as the REST front door, maps methods → Lambda. Optional: custom domain/stage for multiple environments. Expected Results 2 Hello World functions (Node.js/Python) with parameters. 1 REST endpoint (GET/POST) via API Gateway. 1 business logic function (purchase recommendation) returning JSON. Duration and Structure Part 1: Lambda basics, test in console. Part 2: API Gateway, mapping input, testing. Part 3: In-app purchase exercise, cleanup resources. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-api-gateway/5.4.2-add-post/","title":"Create an S3 Interface endpoint","tags":[],"description":"","content":"In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-lambda-basics/5.3.2-hello-python/","title":"Test the Gateway Endpoint","tags":[],"description":"","content":"Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Understand and use AWS Cloud9 IDE basics. Create and manage Cloud9 environment. Practice basic operations on Cloud9. Use AWS CLI in Cloud9 environment. Clean up resources after hands-on practice. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Overview introduction to AWS Cloud9 IDE - Learn main features: code editor, integrated terminal, debugger, support for multiple languages - Benefits when using Cloud9 in labs and projects 10/11/2025 10/11/2025 https://000049.awsstudygroup.com/ 3 - Create new AWS Cloud9 environment - Choose instance type, configure access permissions - Check connection with AWS and access integrated terminal 11/11/2025 11/11/2025 https://000049.awsstudygroup.com/ 4 - Practice basic operations in Cloud9: + Create files and folders + Edit code, save and delete files + Use terminal to run basic commands 12/11/2025 12/11/2025 https://000049.awsstudygroup.com/ 5 - Use AWS CLI in Cloud9 - Check account info, region, and service list - Practice basic operations with EC2, S3 via CLI 13/11/2025 13/11/2025 https://000049.awsstudygroup.com/ 6 - Clean up created resources this week: delete Cloud9 environment, terminate instance - Document entire process and notes when using Cloud9 for subsequent lab weeks 14/11/2025 14/11/2025 https://000049.awsstudygroup.com/ Week 10 Achievements: Understood how to use AWS Cloud9 IDE:\nMain features: code editor, terminal, debugger, support multiple programming languages. Know how to customize interface based on preference (color theme, keybinding, syntax highlight). Practiced creating and managing Cloud9 environment:\nCreate new Cloud9 environment, choose instance type, configure access permissions. Check AWS connection and use integrated terminal. Proficient in basic operations on Cloud9:\nCreate, edit, save and delete files/folders. Use terminal to run basic commands and check environment. Used AWS CLI in Cloud9:\nCheck account info, region, service list. Practice basic operations with EC2, S3 via CLI. Managed and cleaned up resources:\nDelete Cloud9 environment, terminate instance. Document process and notes when using Cloud9 for subsequent labs. Comprehensive skills:\nConfident using Cloud9 to write, run and manage source code on AWS. Combine CLI and IDE usage to deploy labs efficiently. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Understand and practice hosting static web content on S3, accelerate with CloudFront. Configure access permissions and optimize cache for CloudFront distribution. Familiarize with Amazon ElastiCache for Redis: create cluster, manage nodes and shards. Practice scaling cluster, manage parameter groups and run within VPC. Understand steps to clean up resources to avoid cost incurrence. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - CloudFront with S3 – Introduction \u0026amp; Requirements + AWS account + IAM permission with S3 and CloudFront + Expected cost \u0026lt; $1/month - Hands-on + Create S3 bucket + Upload index.html file 17/11/2025 17/11/2025 https://000094.awsstudygroup.com/ 3 - Configure Amazon CloudFront + Create distribution + Link with S3 bucket + Check access to static web content - Clean up resources + Delete CloudFront distribution and S3 bucket when not needed 18/11/2025 18/11/2025 https://000094.awsstudygroup.com/ 4 - Check, optimize \u0026amp; reference + Check content load speed + Optimize cache and access permissions + Reference material: Amazon S3 \u0026amp; CloudFront 19/11/2025 19/11/2025 https://000094.awsstudygroup.com/ 5 - Amazon ElastiCache for Redis – Introduction \u0026amp; Overview + Set up and manage Redis cluster + Integrate with EC2, CloudWatch, CloudTrail, SNS + Automatic error detection and recovery - Clusters \u0026amp; Nodes + Create cluster, choose instance, node type + Data stratification, Standard/Memory-optimized storage type 20/11/2025 20/11/2025 https://000061.awsstudygroup.com/ 6 - ElastiCache for Redis – Advanced Practice + Create and manage shards + Scale node and cluster + Manage parameter group + Run cluster within VPC, subnet, security group + Check number of nodes, shards and available IP addresses 21/11/2025 21/11/2025 https://000061.awsstudygroup.com/ Week 11 Achievements: CloudFront with S3\nUnderstood how S3 and CloudFront work. Created S3 bucket and uploaded index.html file. Configured access permissions for bucket. Created CloudFront distribution and linked S3. Verified access to static web content via CloudFront. Optimized cache and load speed. Cleaned up S3 and CloudFront to avoid costs. Used IAM to assign access permissions. Amazon ElastiCache for Redis\nUnderstood cluster, node and shard concepts. Created Redis cluster and chose appropriate node type. Practiced scaling cluster and shards. Managed basic parameter groups. Deployed cluster within VPC and configured security groups. Verified automatic error recovery capability. Cleaned up cluster after hands-on practice. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Review and consolidate knowledge from previous weeks. Complete workshop. Complete group project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Review learned knowledge: AWS CLI, S3, CloudFront, IAM, SNS, DynamoDB, ElastiCache. 24/11/2025 24/11/2025 3 Complete workshop. 25/11/2025 25/11/2025 4 Complete group project section. 26/11/2025 26/11/2025 5 Continue completing project 27/11/2025 27/11/2025 6 Deploy and test group project on AWS. 28/11/2025 28/11/2025 Week 12 Achievements: Consolidated comprehensive knowledge about AWS CLI, S3, CloudFront, IAM, SNS, DynamoDB, ElastiCache. Completed workshop, proficiently practiced AWS commands and operations. Completed group project, efficiently deployed and managed AWS resources. Enhanced teamwork skills, coordinated real-world project deployment. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.2-prerequiste/","title":"Prerequisites","tags":[],"description":"","content":"Account \u0026amp; Permissions AWS account with permissions to create Lambda, CloudWatch Logs, API Gateway (minimum IAM: AWSLambdaBasicExecutionRole). No advanced EC2/VPC permissions needed for this workshop. Tools Browser for AWS Console operations. curl/Invoke-RestMethod or Postman to call API Gateway. Optional: local editor to write Node.js/Python code before pasting into console. Quick Setup Select a nearby region (e.g., us-east-1 or ap-southeast-1). Create an IAM role for Lambda with trust policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;sts:AssumeRole\u0026#34;], \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: [\u0026#34;lambda.amazonaws.com\u0026#34;] } } ] } Attach the AWSLambdaBasicExecutionRole policy to the role. Set CloudWatch Logs retention (e.g., 7–14 days) to control costs: CloudWatch → Log groups → /aws/lambda/\u0026lt;function-name\u0026gt; → Actions → Edit retention. "},{"uri":"https://thienluhoan.github.io/workshop-template/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Online Library - Serverless Content Platform For Small Groups 1. Executive Summary The Online Library project aims to build a serverless, low-cost platform for storing and distributing content (PDF/ePub) to a small group of users (initially ~100 people, a group comprising students/lab members who need to share moderated internal research documents). This solution prioritizes security, content approval workflow (Admin Approval), and transparent, linear operational costs as it scales. The architecture uses completely AWS Serverless (Amplify, Cognito, API Gateway, Lambda, S3, CloudFront, DynamoDB). Estimated MVP cost (not counting Free Tier) ≈ $9.80/month, ensuring scalability to 5,000 to 50,000 users with predictable costs.\n2. Problem What\u0026rsquo;s the Problem? Documents and books are scattered; lack of a secure content delivery system with access control; adding or content approval processes are time-consuming and involve various legal issues.\nSolution Build a serverless pipeline on AWS: Users upload via Presigned PUT URL (to temporary S3); Admin approves → Lambda moves file to public folder (but protected); Readers access via Signed GET URL (from CloudFront/CDN) to ensure speed and access control.\nBenefits and Return on Investment Business Value: Centralize content; quality control through approval workflow; rapid deployment with CI/CD. Technical Benefits: Low operational cost (≈ $9.80/month at MVP, not counting Free Tier); Serverless architecture can easily scale large ; secure content access. 3. Solution Architecture A. High-level B. Request Processing Flow AWS Services Used Service Primary Role Specific Operations Amplify Hosting CI/CD + FE Hosting Build \u0026amp; Deploy Next.js, manage domain Cognito Authentication Sign up/Login, issue JWT, refresh token API Gateway API Entry Point Receive request, authenticate JWT, route to Lambda Lambda Business Logic Handle upload, approval, create signed URL, write metadata S3 Object Storage Store original file, approved file, downloaded via Cloudfront Signed URL CloudFront CDN Fast content distribution, block direct access via OAC DynamoDB Database Store metadata (book name, uploader, approval status) Route 53 DNS Point domain to Amplify Hosting, API Gateway, CloudFront CloudWatch Monitoring Store Lambda logs, alert on errors or unusual costs Search:\nSimple search by field (ex: book name, author), use DynamoDB GSIs for these attributes and query by GSI. Request Processing Flow User Upload: Presigned PUT to S3 uploads/ folder. Admin Approval: Lambda copies file from uploads/ to public/books/ when approved. Reader Security: CloudFront uses Origin Access Control (OAC) to block direct S3 access and only allow reading via Signed URL (short-lived) created by Lambda. Search Architecture Simple Search: Design GSI for title and author (example: GSI1: PK=TITLE#{normalizedTitle}, SK=BOOK#{bookId}; GSI2: PK=AUTHOR#{normalizedAuthor}, SK=BOOK#{bookId}). Add GET /search?title=...\u0026amp;author=... endpoint to query by GSI instead of Scan. Admin Permissions Use Cognito User Groups with an Admins group in User Pool. When Admin logs in, JWT contains cognito:groups: [\u0026quot;Admins\u0026quot;]. Admin business Lambda functions (e.g., approveBook, takedownBook) must check this claim; if missing group, return 403 Forbidden. Can use JWT Authorizer (API Gateway HTTP API) for authentication, with detailed authorization handled in Lambda based on claims. 4. Technical Implementation Deployment Design \u0026amp; IaC (Infrastructure-as-Code): Build CDK stacks (Cognito, DDB, S3, Amplify, Lambda, API). Upload \u0026amp; Approval Flow: Implement Presigned PUT, save metadata (status: pending), and Admin approval logic (copy file). Book Reading Flow: Implement Signed GET endpoint, and reading interface (FE stream via CloudFront). Operations: Setup CloudWatch logs (short retention), Budget Alerts, IAM hardening. Search: MVP: add GSI for title, author and GET /search endpoint query by GSI. Technical Requirements Use CDK to define entire infrastructure. API Gateway must be HTTP API for cost optimization. Lambda (Python) handles business logic and DynamoDB/S3 interactions. S3 Bucket Policy must block public access and only allow CloudFront OAC. 5. Timeline and Milestones Project Timeline Foundation \u0026amp; Authentication (Weeks 1-2) Goal: set up infrastructure and enable user login.\nBackend Tasks (CDK/DevOps): Write CDK stack for Cognito (User Pool, App Client). Write CDK stack for DynamoDB (main table, no GSI yet). Write CDK stack for S3 (Buckets uploads, public, logs) and configure OAC (Origin Access Control). Deploy API Gateway (HTTP API) and a \u0026ldquo;hello world\u0026rdquo; Lambda to test. Frontend Tasks (Amplify): Configure Amplify Hosting and connect to GitHub repo (CI/CD). Integrate Amplify UI / Cognito SDK for pages: Sign Up, Email Verification, Login, Forgot Password. Milestone: Developer can git push and FE auto-deploys. Users can sign up/login and receive JWT token. Upload \u0026amp; Approval Flow (Weeks 2-3) Goal: enable signed-in users to upload files and Admin to approve them.\nBackend Tasks (CDK/Lambda): Write Lambda createUploadUrl: Authenticate JWT (must be logged in). Create Presigned PUT URL pointing to S3 uploads/ folder. Write metadata to DynamoDB (status: PENDING). Write Lambda approveBook: Authenticate JWT (must be Admin). Copy file from uploads/ to public/books/. Update status in DynamoDB (status: APPROVED). Frontend Tasks: Build Upload Form (drag-drop, file selection). Call API createUploadUrl to get URL. Perform file upload (HTTP PUT) directly to S3 Presigned URL. Build Admin Interface: Get list of books with status PENDING. Have \u0026ldquo;Approve\u0026rdquo; button (calls API approveBook). Reading \u0026amp; Search Flow (Weeks 3-4) Goal: enable users to read and search approved books.\nBackend Tasks (CDK/Lambda): Write Lambda getReadUrl: Authenticate JWT (must be logged in). Check book has status APPROVED. Create Signed GET URL (short-lived) via CloudFront pointing to file in public/books/. Update CDK: Add GSI (Global Secondary Index) for title and author to DynamoDB table. Write Lambda searchBooks: Query DynamoDB by GSI (no Scan). Frontend Tasks: Build Home Page: Display book list (from API, no URLs). Build Search Bar (call API searchBooks). Build Reading Interface (Reader): When clicking \u0026ldquo;Read\u0026rdquo;, call API getReadUrl. Use received URL to render file (e.g., use react-pdf). Operations \u0026amp; Security (Weeks 5-6) Goal: harden the system, make it safe and easy to monitor.\nBackend Tasks (CDK/Lambda): Setup S3 Event Notification (for uploads/). Write Lambda validateMimeType: Trigger on new file, read \u0026ldquo;magic bytes\u0026rdquo; to validate PDF/ePub. If wrong, update status: REJECTED_INVALID_TYPE. Write Lambda takedownBook (API for Admin) and deleteUpload (delete PENDING files after 72h). DevOps Tasks (AWS Console/CDK): Setup AWS Budget Alerts (alert when cost exceeds $X). Setup CloudWatch Alarms (e.g.: Lambda error rate \u0026gt; 5%). Review IAM (ensure \u0026ldquo;least-privilege\u0026rdquo;), CORS (only allow Amplify domain). 6. Budget Estimation You can find the budget estimation on the: AWS Pricing Calculator\nBelow is a strict monthly cost estimate (assuming no AWS Free Tier applied) at MVP scale (100 users).\n# AWS Service Region Monthly (USD) Notes 0 Amazon CloudFront Asia Pacific (Singapore) 0.86 10 GB data egress + 10,000 HTTPS requests 1 AWS Amplify Asia Pacific (Singapore) 1.31 100 build min + 0.5 GB storage + 2 GB served 2 Amazon API Gateway Asia Pacific (Singapore) 0.01 ~10,000 HTTP API calls/month 3 AWS Lambda Asia Pacific (Singapore) 0.00 128 MB RAM × 100 ms × 10,000 invokes 4 Amazon S3 (Standard) Asia Pacific (Singapore) 0.05 2 GB object storage for books/images 5 Data Transfer Asia Pacific (Singapore) 0.00 Included in CloudFront cost 6 DynamoDB (On-Demand) Asia Pacific (Singapore) 0.03 Light metadata table (0.1 GB, few reads/writes) 7 Amazon Cognito Asia Pacific (Singapore) 5.00 100 MAU, Advanced Security enabled 8 Amazon CloudWatch Asia Pacific (Singapore) 1.64 5 metrics + 0.1 GB logs/month 9 Amazon Route 53 Asia Pacific (Singapore) 0.90 1 Hosted Zone + DNS queries ≈ 9.80 USD / month No Free Tier applied Infrastructure Cost This cost model demonstrates serverless architecture efficiency: costs focus mainly on value delivered to users (Cognito MAU) rather than paying for \u0026ldquo;idle servers\u0026rdquo;.\n7. Risk Assessment Risk Matrix Risk Impact Mitigation Strategy Cost increases with user surge High Limit MAU, cache metadata via CloudFront Upload abuse Medium Limit ≤ 50MB/file, auto-delete after 72h Fake/malicious files Medium S3 Event → Lambda validate MIME (magic bytes) Monitoring overload Low CloudWatch alert, 14-day log retention Mitigation Strategies Costs: Set AWS Budget Alerts for CloudFront and Cognito. Understand Signed URL has short TTL so no long-term public cache; instead, cache metadata/API response (book list, details) on CloudFront 3–5 min to reduce API load. Only create Signed URL when user actually clicks read (on‑demand), not pre-create for entire list. Uploads: Limit file size ≤ 50MB for MVP. (Can increase to 200MB later, use multipart upload in FE to avoid timeout.) Apply Rate Limit/Throttling on API Gateway for Presigned URL creation endpoints. Set S3 Lifecycle Policy to auto-delete unapproved files in uploads/ after 72h. Add Server‑side Validation: S3 Event Notifications → Lambda read magic bytes (e.g., file-type library) to validate PDF/ePub; if invalid, auto-delete and set status REJECTED_INVALID_TYPE in DynamoDB. Copyright (DMCA): Store Audit Log in DynamoDB: uploaderID, uploadTimestamp, adminApproverID, approvalTimestamp for traceability. Build Takedown API (Admin only): update status TAKEDOWN; optionally move object from public/books/ to quarantine/books/ (don\u0026rsquo;t delete) to keep records. Contingency Plan If costs exceed budget, can temporarily limit new users via invite system to control Cognito MAU and optimize file size.\n8. Expected Results Technical Improvements: Ensure fast content delivery speed and security (CDN + Signed URL). Create a standard Serverless architecture on AWS, easily scalable to 50,000 users without core architecture changes. Fully automated CI/CD for both Frontend and Backend (CDK/Amplify). Long-term Value Establish a centralized, structured data platform for book content. Provide a living reference documentation for E2E Serverless implementation. Potential to integrate analytics services (like Amazon QuickSight) or AI/ML in the future. This system demonstrates the ability to build secure, cost-efficient, and easily scalable content platforms using AWS Serverless — suitable for real-world deployment in small groups.\n9. Attached Documents Online Library "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-lambda-basics/","title":"Lambda Basics","tags":[],"description":"","content":"Objectives Create and test Lambda Hello World (Node.js/Python), pass input parameters, and fine-tune configuration (memory, timeout, logs).\nKey Steps Create a Node.js Function\nRuntime: Node.js 18.x (example).\nSample handler returns JSON { message: \u0026quot;hello\u0026quot; }.\nTest event with name parameter, log results in CloudWatch. Create a Python Function\nRuntime: Python 3.12 (example). Handler reads event[\u0026quot;name\u0026quot;] and returns customized response. Parameterization \u0026amp; Common Errors\nRead event (query/body will be added by API Gateway in the next step). Catch missing key errors, return appropriate status codes. Light Optimization\nAdjust Memory and Timeout to balance cost/performance. Set CloudWatch retention. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-api-gateway/5.4.3-test-api/","title":"Test the Interface Endpoint","tags":[],"description":"","content":"Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/","title":"Translated Blog Posts","tags":[],"description":"","content":"This section lists and introduces the blog posts that have been translated.\nBlog 1 - Announcing Capacity Blocks Support for AWS Parallel Computing Service This blog introduces the support for Amazon EC2 Capacity Blocks in AWS Parallel Computing Service (PCS), enabling users to reserve and schedule GPU instances for AI/ML and HPC workloads. You will learn what Capacity Blocks are, why they are useful for model training, inference, and scientific simulation tasks, and how PCS leverages this feature to provide a flexible, scalable, and reliable computing environment. The article also guides you through the process of setting up Capacity Blocks in PCS — from purchasing blocks, creating launch templates, configuring compute node groups — to best practices for operations, as well as pricing information, supported regions, and monitoring events to ensure seamless operations.\nBlog 2 - Announcing Amazon EC2 M4 and M4 Pro Mac instances This blog introduces the Amazon EC2 M4 and M4 Pro Mac instances — a new generation for Apple application development teams requiring high build \u0026amp; test performance. You will learn how Apple silicon M4/M4 Pro delivers improved build speeds (up to 20% faster than M2 Mac), larger unified memory, and 2 TB of local storage to accelerate caching. The article also guides you through allocating dedicated hosts, choosing appropriate macOS AMIs, configuring storage, and installing Xcode in the EC2 environment. Additionally, the blog shares important considerations for operating EC2 Mac, CI/CD integration options with AWS, and pricing \u0026amp; region availability information, helping Apple development teams modernize their workflows on AWS in a flexible and efficient manner.\nBlog 3 - Harnessing the Power of AWS IoT Rules with Substitution Templates This blog introduces how to leverage substitution templates in AWS IoT Rules to optimize IoT architecture. You will learn how AWS IoT Core and the rules engine use SQL-like syntax to filter, transform, and route data to over 20 AWS services. The article explains why substitution templates are the \u0026ldquo;secret weapon\u0026rdquo; that simplifies architecture, reduces Lambda dependencies, cuts costs, and increases scalability. The blog also illustrates 3 real-world scenarios: routing messages by partnerId from IoT Registry, intelligent load balancing across multiple Firehose streams using rand() + mod functions, and dynamic routing to different Lambda functions using CASE statements. Through these examples, you will see how to build flexible, serverless IoT solutions that are easier to maintain thanks to the ability to dynamically insert values directly in AWS IoT Rule actions.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-api-gateway/","title":"API Gateway","tags":[],"description":"","content":"Objectives Publish Lambda via API Gateway (REST), create resources/methods, and test GET/POST using curl or Postman.\nSteps Create REST API, resource /hello, enable CORS if needed. Add GET/POST methods, map to Lambda (proxy integration). Deploy stage dev, note the Invoke URL. Test GET/POST, handle errors (400) when data is missing. Optional: enable CORS or custom domain. "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/","title":"Events Attended","tags":[],"description":"","content":"During my internship, I attended 2 events, each of which was a memorable experience with new, valuable, and useful knowledge, along with prizes and wonderful moments.\nEvent 1 Event Name: AWS CloudDay 2025\nTime: 09:00 on September 18, 2025\nLocation: Floor 26, Bitexco Building, 02 Hai Trieu Street, District 1, Ho Chi Minh City\nRole in Event: Attendee\nEvent 2 Event Name: Generative AI with Amazon Bedrock\nTime: 8:30 AM on November 15, 2025\nLocation: Floor 26, Bitexco Building, 02 Hai Trieu Street, District 1, Ho Chi Minh City\nRole in Event: Attendee\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-api-gateway/5.4.4-cors-custom-domain/","title":"On-premises DNS Simulation","tags":[],"description":"","content":"AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Overview A serverless workshop with AWS Lambda and API Gateway: creating a Hello World function (Node.js/Python), adding parameters, publishing GET/POST APIs, and building an in-app purchase recommendation function. Focus on rapid deployment, serverless approach, cost consciousness, and operations.\nContent Overview Prerequisites Lambda Basics API Gateway Sample App (Recommendation Function) Cleanup "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-sample-app/","title":"Sample App: In-App Purchase Recommendation","tags":[],"description":"","content":"Exercise: In-App Purchase Recommendation Build a Lambda function that receives a list of purchased items and returns recommendations for unpurchased items (simple set subtraction).\nInput Requirements { \u0026#34;allItems\u0026#34;: [\u0026#34;starter_pack\u0026#34;,\u0026#34;booster_pack\u0026#34;,\u0026#34;data_pack\u0026#34;,\u0026#34;golden_apples\u0026#34;,\u0026#34;skins\u0026#34;], \u0026#34;owned\u0026#34;: [\u0026#34;data_pack\u0026#34;,\u0026#34;starter_pack\u0026#34;] } Processing Use sets to get the remaining items: suggestions = allItems - owned. Return JSON { \u0026quot;suggestions\u0026quot;: [...] } with status 200. Log input/output for easy debugging. Testing In console: create test event with multiple owned values. Via API Gateway: POST /suggest with JSON body as above. Try error cases: missing allItems or owned → return 400 with clear message. "},{"uri":"https://thienluhoan.github.io/workshop-template/6-self-evaluation/","title":"Self-Evaluation","tags":[],"description":"","content":"Throughout my internship at [Amazon Web Service company] from [September 8, 2025] to [November 28, 2025], I had the opportunity to learn, practice, and apply the knowledge I acquired at school to a real-world work environment. I participated in the [First Cloud Journey program organized by AWS company], through which I learned many skills such as [teamwork, acquiring new knowledge, and gaining the opportunity to experience a professional work environment].\nRegarding work style, I always tried to complete tasks well, comply with regulations, and actively communicate with colleagues and mentors to improve work efficiency.\nTo objectively reflect the internship process, I would like to self-evaluate based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional Knowledge \u0026amp; Skills Industry understanding, applying knowledge to practice, tool usage skills, work quality ☐ ✅ ☐ 2 Learning Ability Acquiring new knowledge, learning quickly ✅ ☐ ☐ 3 Initiative Self-research, accepting tasks without waiting for guidance ✅ ☐ ☐ 4 Sense of Responsibility Completing work on time, ensuring quality ✅ ☐ ☐ 5 Discipline Following schedules, regulations, and work procedures ✅ ☐ ☐ 6 Aspiration for Progress Ready to receive feedback and self-improvement ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ☐ ✅ 8 Team Collaboration Working effectively with colleagues, team participation ✅ ☐ ☐ 9 Professional Conduct Respecting colleagues, partners, and work environment ✅ ☐ ☐ 10 Problem-Solving Mindset Identifying issues, proposing solutions, creativity ☐ ✅ ☐ 11 Contribution to Project/Organization Work efficiency, improvement initiatives, team recognition ☐ ✅ ☐ 12 Overall General assessment of the entire internship process ☐ ✅ ☐ Areas for Improvement Improve communication skills with team members and public speaking skills Need to learn more professional work practices Need to learn how to handle certain situations more professionally Need to acquire more knowledge "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-cleanup/","title":"Cleanup Resources","tags":[],"description":"","content":"Resource Cleanup Congratulations on completing this lab! In this lab, you learned about architectural patterns to access Amazon S3 without using the Public Internet.\nBy creating a Gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, bypassing the Internet Gateway. By creating an Interface endpoint, you extended S3 connectivity to resources running in your on-premises data center through AWS Site-to-Site VPN or Direct Connect. Cleanup Steps Navigate to Hosted Zones on the left side of the Route 53 console. Click on the name of the s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by entering the keyword \u0026ldquo;delete\u0026rdquo;. Disassociate Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and delete it. Open the CloudFormation console and delete the two CloudFormation stacks you created for this lab: PLOnpremSetup PLCloudSetup Delete the S3 buckets Open the S3 console Select the bucket we created for the lab, click it and confirm it is empty. Click Delete and confirm deletion. "},{"uri":"https://thienluhoan.github.io/workshop-template/7-feedback/","title":"Feedback &amp; Contributions","tags":[],"description":"","content":"General Assessment 1. Work Environment\nThe atmosphere at FCJ is very comfortable and friendly. Everyone is approachable and always ready to help, even with questions after work hours when I face difficulties. The spacious, neat, and clean work space helps me concentrate better. The company frequently organizes interesting and beneficial events that help me learn new knowledge from those who came before.\n2. Support from Mentor / Team Admin\nMy mentor guided me very dedicatedly, explaining clearly every part I didn\u0026rsquo;t understand and always encouraging me to ask questions freely. Especially at the beginning when I faced difficulties creating an AWS account, the team members helped me very enthusiastically, and during the final project, everyone supported my team tremendously.\n3. Alignment between Work and Academic Major\nAlthough I studied Artificial Intelligence at school, when I interned at FCJ, I was exposed to the Cloud field — a field quite different from what I had previously learned. However, this actually helped me broaden my perspective and learn a lot of completely new knowledge that I hadn\u0026rsquo;t had the opportunity to explore before. As a result, I not only supplemented practical skills but also gained a better understanding of how the systems behind the AI applications I studied in class actually work.\n4. Learning Opportunities \u0026amp; Skill Development\nDuring my internship, I learned many things: how to use project management tools, how to collaborate in a team, and how to communicate more professionally in a corporate environment. My mentor also shared many real-world experiences, helping me better understand what working in a corporate environment is like.\n5. Culture \u0026amp; Team Spirit\nI really like the culture here. Everyone respects each other, works seriously but maintains enjoyment. When teammates face account or project issues, everyone helps each other to the fullest, regardless of position or role. This made me feel that I\u0026rsquo;m not alone in the corporate environment.\n6. Internship Policies / Benefits\nThe company and school have created a good internship environment and flexible working hours when I have personal matters. Additionally, being able to participate in company-organized events helped me learn many valuable things — this is a big plus for me.\nAdditional Questions What are you most satisfied with during your internship? -\u0026gt; What I\u0026rsquo;m most satisfied with during my internship is the spirit of FCJ team members. Everyone has such wonderful positive energy that made me feel comfortable even though it was our first meeting, making my internship experience very great. Especially those fruits that brother Kha bought for us every afternoon.\nWhat do you think the company needs to improve for future interns? -\u0026gt; After 3 months of internship based on my personal perception, the company has nothing that needs improvement for future interns.\nIf you recommend to your friends, would you recommend them to intern here? Why? -\u0026gt; If I have friends, I will recommend everyone to intern here. Because I think beyond the excellent internship environment, knowledge is also a very important part, and this is knowledge that is quite necessary for people like us who study information technology, and I think everyone should try to get in touch with it to see if they\u0026rsquo;re suited for this field, you never know if it might be the door that opens a path to a new horizon for everyone.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://thienluhoan.github.io/workshop-template/tags/","title":"Tags","tags":[],"description":"","content":""}]